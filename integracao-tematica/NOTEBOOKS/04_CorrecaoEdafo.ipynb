{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Script para corrigir a base de Aptid\u00e3o Solo por Estado\n",
    "\n",
    "Projeto: Sistema de Apoio \u00e0 Caracteriza\u00e7\u00e3o de Im\u00f3veis Rurais  \n",
    "Embrapa/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# M\u00f3dulos necess\u00e1rios\n",
    "import os\n",
    "import glob\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udea8 Substitu\u00eddo automaticamente\n",
    "import os\n",
    "# Original: # \ud83d\udea8 Substitu\u00eddo automaticamente\n# import os\n# # Original: # Definir diret\u00f3rio principal\n# # dirpath = input('Diret\u00f3rio principal: ')\n# dirpath = os.getenv('INPUT_PATH', '/app/input')\n# \n",
    "dirpath = os.getenv('INPUT_PATH', '/app/input')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udea8 Substitu\u00eddo automaticamente\n",
    "import os\n",
    "# Original: # \ud83d\udea8 Substitu\u00eddo automaticamente\n# import os\n# # Original: # Definir as pastas com a aptid\u00e3o solo por UF\n# # originais_path = input('Pasta com originais: ')\n# originais_path = os.path.join(os.getenv('INPUT_PATH', '/app/input'), 'originais_path')\n# \n",
    "originais_path = os.path.join(os.getenv('INPUT_PATH', '/app/input'), 'originais_path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \ud83d\udea8 Substitu\u00eddo automaticamente\n",
    "import os\n",
    "# Original: # \ud83d\udea8 Substitu\u00eddo automaticamente\n# import os\n# # Original: # Definir caminho do shapefile que ser\u00e1 usado como m\u00e1scara para recorte (Munic\u00edpios do BR - sem Buffer)\n# # limites = input('Caminho Munic\u00edpios: ')\n# mun_path = os.path.join(os.getenv('INPUT_PATH', '/app/input'), 'Municipios', 'BR_Municipios_2021.shp')\n# \n",
    "mun_path = os.path.join(os.getenv('INPUT_PATH', '/app/input'), 'Municipios', 'BR_Municipios_2021.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo shapefile de Munic\u00edpios do BR\n",
    "lim = gpd.read_file(limites)\n",
    "# Reprojetando o shapefile para WGS84\n",
    "lim = lim.to_crs(epsg=4326)\n",
    "# Criar pasta para armazenar Aptid\u00e3o Solo corrigida\n",
    "out_path = os.path.join(dirpath, 'Aptidao_Solo_Corrigida')\n",
    "os.makedirs(out_path, exist_ok=True)\n",
    "# Caminhos dos vetores de aptid\u00e3o solo\n",
    "vetores = glob.glob(originais_path + f'**/*.shp')\n",
    "# Agrupar por Estado para ser poss\u00edvel iterar sob as fei\u00e7\u00f5es\n",
    "agrupado = lim.groupby('SIGLA_UF')\n",
    "for vetor in vetores:\n",
    "    for key, values in agrupado:\n",
    "        if key in vetor:\n",
    "            aptidao = gpd.read_file(vetor)\n",
    "            output = out_path + f'\\Aptd_Edafo_{key}_temp1.shp'\n",
    "            estado = lim[lim['SIGLA_UF'] == f\"{key}\"]\n",
    "            erase = estado.overlay(aptidao, how='difference', keep_geom_type=True)\n",
    "            single = erase.explode(ignore_index=True)\n",
    "            single.to_file(driver = 'ESRI Shapefile', filename = (rf'{output}'))\n",
    "vetores_erase = glob.glob(out_path + f'**/*temp1.shp')\n",
    "vetores_aptidao = glob.glob(originais_path + f'**/*.shp')\n",
    "caminho_erase = []\n",
    "caminho_aptidao = []\n",
    "for arquivo in vetores_erase:\n",
    "    caminho = os.path.join(out_path,arquivo)\n",
    "    caminho_erase.append(caminho)\n",
    "for arquivo in vetores_aptidao:\n",
    "    caminho = os.path.join(originais_path,arquivo)\n",
    "    caminho_aptidao.append(caminho)\n",
    "vetores = caminho_erase + caminho_aptidao\n",
    "geodf = pd.concat([\n",
    "    gpd.read_file(vetor)\n",
    "    for vetor in vetores\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "for key, values in agrupado:\n",
    "    estado = lim[lim['SIGLA_UF'] == f\"{key}\"]\n",
    "    output = out_path + f'\\Aptd_Edafo_{key}_temp2.shp'\n",
    "    geodf_clip = gpd.clip(geodf, estado, keep_geom_type=True)\n",
    "    geodf_clip.to_file(driver = 'ESRI Shapefile', filename = rf'{output}')\n",
    "# Preenchendo os valores null com valores pr\u00f3ximos\n",
    "vetores_temp2 = glob.glob(out_path + f'**/*temp2.shp')\n",
    "for arquivo in vetores_temp2:\n",
    "    temp2 = gpd.read_file(arquivo)\n",
    "    nome_arq = os.path.basename(arquivo).replace('temp2.shp', 'temp3.shp')\n",
    "    output = out_path + f'\\{nome_arq}'\n",
    "    null = temp2[temp2['APTD_EDAFO'].isna()]\n",
    "    not_null = temp2[temp2['APTD_EDAFO'].notna()]\n",
    "    gdfspatial  = null.sjoin_nearest(not_null, max_distance=5)\n",
    "    gdfspatial.to_file(driver = 'ESRI Shapefile', filename = (rf'{output}'))\n",
    "# Merge das fei\u00e7\u00f5es com null preenchido e shape original de edafo\n",
    "vetores_temp3 = glob.glob(out_path + f'**/*temp3.shp')\n",
    "caminho_temp3 = []\n",
    "for arquivo in vetores_temp3:\n",
    "    caminho = os.path.join(out_path,arquivo)\n",
    "    caminho_temp3.append(caminho)\n",
    "vetores = caminho_temp3 + caminho_aptidao\n",
    "geodf = pd.concat([\n",
    "    gpd.read_file(vetor)\n",
    "    for vetor in vetores\n",
    "]).pipe(gpd.GeoDataFrame)\n",
    "for key, values in agrupado:\n",
    "    estado = lim[lim['SIGLA_UF'] == f\"{key}\"]\n",
    "    output = out_path + f'\\Aptd_Edafo_{key}_temp4.shp'\n",
    "    geodf_clip = gpd.clip(geodf, estado, keep_geom_type=True)\n",
    "    geodf_clip.to_file(driver = 'ESRI Shapefile', filename = rf'{output}')\n",
    "# Exportando apenas colunas necess\u00e1rias\n",
    "vetores_temp4 = glob.glob(out_path + f'**/*temp4.shp')\n",
    "for vetor in vetores_temp4:\n",
    "    temp4 = gpd.read_file(vetor)\n",
    "    colunas = list(temp4.columns)\n",
    "    colunas = [coluna for coluna in colunas if 'EDA' not in coluna and 'LEG' not in coluna and 'geometry' not in coluna]\n",
    "    temp4 = temp4.drop(colunas, axis = 1).pipe(gpd.GeoDataFrame)\n",
    "    temp4['APTD_EDAFO'].fillna(temp4['APTD_EDA_1'], inplace=True)\n",
    "    temp4['CD_EDAFO'].fillna(temp4['CD_EDAFO_r'], inplace=True)\n",
    "    temp4['CD_EDAFO'].fillna(temp4['CD_EDAFO_l'], inplace=True)\n",
    "    temp4['LEG_SOLO'].fillna(temp4['LEG_SOLO_r'], inplace=True)\n",
    "    temp4['LEG_SOLO'].fillna(temp4['LEG_SOLO_l'], inplace=True)\n",
    "    remover = ['APTD_EDA_1', 'CD_EDAFO_r', 'CD_EDAFO_l', 'LEG_SOLO_r', 'LEG_SOLO_l']\n",
    "    temp4 = temp4.drop(remover, axis = 1).pipe(gpd.GeoDataFrame)\n",
    "    nome = os.path.basename(vetor).replace('_temp4.shp', '_corrigido.shp')\n",
    "    output = out_path + f\"\\{nome}\"\n",
    "    temp4.to_file(driver = 'ESRI Shapefile', filename = rf'{output}')\n",
    "# Remover os arquivos tempor\u00e1rios da pasta\n",
    "time.sleep(5)\n",
    "def excluir_arquivos_temporarios(pasta):\n",
    "    for nome_arquivo in os.listdir(pasta):\n",
    "        if \"temp\" in nome_arquivo:\n",
    "            caminho_arquivo = os.path.join(pasta, nome_arquivo)\n",
    "            if os.path.isfile(caminho_arquivo):\n",
    "                os.remove(caminho_arquivo)\n",
    "                print(f\"Arquivo exclu\u00eddo: {caminho_arquivo}\")\n",
    "diretorio = out_path\n",
    "excluir_arquivos_temporarios(diretorio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}